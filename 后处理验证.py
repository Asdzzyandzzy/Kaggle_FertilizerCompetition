import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('ggplot')
pd.set_option('display.max_columns', 50)

def feature_engineering_complete(df, train_df=None):
    """
    å®Œæ•´ç‰ˆç‰¹å¾å·¥ç¨‹ - åŒ…å«æ‰€æœ‰é«˜é˜¶ç‰¹å¾
    
    å‚æ•°:
    df -- è¦å¤„ç†çš„æ•°æ®æ¡†
    train_df -- ç”¨äºè®¡ç®—ç»Ÿè®¡é‡çš„è®­ç»ƒé›† (é»˜è®¤ä¸ºNone)
    """
    # ä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡ï¼ˆå¦‚æœæä¾›ï¼‰
    if train_df is None:
        train_df = df.copy()
    
    # æ·±æ‹·è´æ•°æ®
    df = df.copy()
    
    # === ğŸŒ¿ åŸºç¡€åˆ†ç®±ç‰¹å¾ ===
    df["Temp_bin"] = pd.cut(df["Temparature"], bins=[0,20,25,30,35,40,100], 
                           labels=[0,1,2,3,4,5]).astype("int8")
    
    df["Humidity_bin"] = pd.cut(df["Humidity"], bins=5, labels=False).astype("int8")
    df["Moisture_bin"] = pd.cut(df["Moisture"], bins=5, labels=False).astype("int8")
    
    # === ğŸŒ¿ ç±»åˆ«ç»„åˆç‰¹å¾ ===
    df["Soil_Crop"] = df["Soil Type"] + "_" + df["Crop Type"]
    df["Soil_Crop_Temp"] = df["Soil_Crop"] + "_" + df["Temp_bin"].astype(str)
    
    # === ğŸŒ¿ NPK æ¯”ç‡ç‰¹å¾ ===
    df["N_P_ratio"] = df["Nitrogen"] / (df["Phosphorous"] + 1e-6)
    df["N_K_ratio"] = df["Nitrogen"] / (df["Potassium"] + 1e-6)
    df["P_K_ratio"] = df["Phosphorous"] / (df["Potassium"] + 1e-6)
    
    # === ğŸŒ¿ NPK ç»„åˆç‰¹å¾ ===
    df["NP_sum"] = df["Nitrogen"] + df["Phosphorous"]
    df["NK_sum"] = df["Nitrogen"] + df["Potassium"]
    df["PK_sum"] = df["Phosphorous"] + df["Potassium"]
    df["NPK_sum"] = df["Nitrogen"] + df["Phosphorous"] + df["Potassium"]
    df["NPK_mean"] = df[["Nitrogen", "Phosphorous", "Potassium"]].mean(axis=1)
    df["NPK_std"] = df[["Nitrogen", "Phosphorous", "Potassium"]].std(axis=1)
    df["NPK_skew"] = df[["Nitrogen", "Phosphorous", "Potassium"]].skew(axis=1)
    
    # === ğŸŒ¿ ç¯å¢ƒäº¤äº’ç‰¹å¾ ===
    df["Temp_Humidity"] = df["Temparature"] * df["Humidity"] / 100
    df["Temp_Moisture"] = df["Temparature"] * df["Moisture"] / 100
    df["Humidity_Moisture"] = df["Humidity"] * df["Moisture"] / 100
    df["Env_Index"] = df["Temparature"] + df["Humidity"] - df["Moisture"]
    
    # === ğŸŒ¿ ä½œç‰©å’ŒåœŸå£¤çš„ç±»åˆ«ç‰¹å¾ ===
    df["High_N_crop"] = df["Crop Type"].isin(["Sugarcane", "Tobacco", "Cotton"]).astype(int)
    df["High_P_crop"] = df["Crop Type"].isin(["Paddy", "Barley", "Wheat"]).astype(int)
    df["High_K_crop"] = df["Crop Type"].isin(["Fruits", "Vegetables"]).astype(int)
    
    soil_mapping = {"Sandy": 0, "Loamy": 1, "Black": 2, "Red": 3}
    df["Soil_encoded"] = df["Soil Type"].map(soil_mapping)
    
    # === ğŸŒ¿ ç±»åˆ«ç»Ÿè®¡ç‰¹å¾ ===
    for col in ["Soil Type", "Crop Type", "Soil_Crop"]:
        for feat in ["Nitrogen", "Phosphorous", "Potassium"]:
            # ä»…å½“åˆ†ç»„æœ‰è¶³å¤Ÿæ•°æ®æ—¶æ‰è®¡ç®—
            if len(train_df[col].unique()) > 1:
                mean_values = train_df.groupby(col)[feat].mean()
                std_values = train_df.groupby(col)[feat].std().fillna(0)
                df[f"{col}_{feat}_mean"] = df[col].map(mean_values)
                df[f"{col}_{feat}_std"] = df[col].map(std_values)
    
    # === ğŸŒ¿ æ—¶é—´åºåˆ—ç‰¹å¾ (å¦‚æœæœ‰æ—¶é—´åˆ—) ===
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        df['Month'] = df['Date'].dt.month
        df['Season'] = df['Month'].apply(lambda m: (m%12+3)//3)  # 1:å†¬, 2:æ˜¥, 3:å¤, 4:ç§‹
        df['Is_rainy_season'] = df['Month'].isin([6,7,8,9]).astype(int)
    
    # === ğŸŒ¿ å¤šé¡¹å¼ç‰¹å¾ ===
    poly_features = ['Temparature', 'Humidity', 'Moisture', 'NPK_sum']
    for feat in poly_features:
        df[f"{feat}_squared"] = df[feat] ** 2
        df[f"{feat}_log"] = np.log1p(df[feat])
    
    return df

def evaluate_features(df, target_columns, models=['lgb', 'xgb', 'cat'], sample_size=None):
    """
    å¤šæ¨¡å‹ç‰¹å¾è¯„ä¼° - æ”¯æŒLightGBM, XGBoostå’ŒCatBoost
    
    å‚æ•°:
    df -- åŒ…å«ç‰¹å¾å’Œç›®æ ‡çš„æ•°æ®æ¡†
    target_columns -- ç›®æ ‡å˜é‡åˆ—ååˆ—è¡¨
    models -- è¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ (é»˜è®¤åŒ…å«æ‰€æœ‰)
    sample_size -- é‡‡æ ·å¤§å° (é»˜è®¤ä¸ºNoneï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®)
    """
    # é‡‡æ ·æ•°æ®ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if sample_size and len(df) > sample_size:
        df = df.sample(sample_size, random_state=42)
    
    # å‡†å¤‡æ•°æ®
    X = df.drop(columns=target_columns)
    y = df[target_columns]
    
    # åˆ†å‰²è®­ç»ƒé›†
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # ç»“æœå®¹å™¨
    results = {}
    
    # === LightGBM ===
    if 'lgb' in models:
        import lightgbm as lgb
        
        # åˆ›å»ºæ•°æ®é›†
        train_data = lgb.Dataset(X_train, label=y_train.iloc[:,0])
        
        # è®­ç»ƒæ¨¡å‹
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'verbose': -1
        }
        lgb_model = lgb.train(params, train_data, num_boost_round=100)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = lgb_model.feature_importance(importance_type='gain')
        lgb_importance = pd.DataFrame({
            'Feature': X.columns,
            'Importance': importance,
            'Model': 'LightGBM',
            'Target': target_columns[0]
        })
        
        # æ·»åŠ åˆ°ç»“æœ
        results['LightGBM'] = lgb_importance
    
    # === XGBoost ===
    if 'xgb' in models:
        import xgboost as xgb
        
        # è®­ç»ƒæ¨¡å‹
        xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)
        xgb_model.fit(X_train, y_train.iloc[:,0])
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = xgb_model.feature_importances_
        xgb_importance = pd.DataFrame({
            'Feature': X.columns,
            'Importance': importance,
            'Model': 'XGBoost',
            'Target': target_columns[0]
        })
        
        # æ·»åŠ åˆ°ç»“æœ
        results['XGBoost'] = xgb_importance
    
    # === CatBoost ===
    if 'cat' in models:
        from catboost import CatBoostRegressor
        
        # è¯†åˆ«ç±»åˆ«ç‰¹å¾
        cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # è®­ç»ƒæ¨¡å‹
        cat_model = CatBoostRegressor(
            iterations=100, 
            verbose=False,
            cat_features=cat_features,
            random_state=42
        )
        cat_model.fit(X_train, y_train.iloc[:,0])
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = cat_model.get_feature_importance()
        cat_importance = pd.DataFrame({
            'Feature': X.columns,
            'Importance': importance,
            'Model': 'CatBoost',
            'Target': target_columns[0]
        })
        
        # æ·»åŠ åˆ°ç»“æœ
        results['CatBoost'] = cat_importance
    
    # === ç›¸å…³æ€§åˆ†æ ===
    corr_matrix = df.corr()
    target_corrs = corr_matrix[target_columns]
    
    # === å¤šé‡å…±çº¿æ€§åˆ†æ ===
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X.columns
    
    # è®¡ç®—VIFéœ€è¦æ•°å€¼å‹æ•°æ®
    num_cols = X.select_dtypes(include=np.number).columns
    vif_data["VIF"] = [variance_inflation_factor(X[num_cols].values, i) 
                      for i in range(len(num_cols))]
    
    # === å¯è§†åŒ–ç»“æœ ===
    plt.figure(figsize=(16, 20))
    
    # ç‰¹å¾é‡è¦æ€§å›¾
    if results:
        all_importance = pd.concat(results.values())
        plt.subplot(3, 1, 1)
        sns.barplot(
            data=all_importance.sort_values('Importance', ascending=False).head(20),
            x='Importance', 
            y='Feature',
            hue='Model'
        )
        plt.title('Top 20 ç‰¹å¾é‡è¦æ€§ (å¤šæ¨¡å‹æ¯”è¾ƒ)')
        plt.tight_layout()
    
    # ç›®æ ‡ç›¸å…³æ€§å›¾
    plt.subplot(3, 1, 2)
    sns.heatmap(target_corrs, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('ç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§')
    
    # VIFå›¾
    plt.subplot(3, 1, 3)
    high_vif = vif_data[vif_data['VIF'] > 5].sort_values('VIF', ascending=False)
    sns.barplot(data=high_vif, x='VIF', y='Feature', palette='Reds_r')
    plt.axvline(x=5, color='r', linestyle='--')
    plt.axvline(x=10, color='r', linestyle='--')
    plt.title('é«˜VIFç‰¹å¾ (VIF > 5)')
    
    plt.tight_layout()
    plt.show()
    
    # === è¿”å›ç»“æœ ===
    return {
        "feature_importance": all_importance if results else None,
        "target_correlation": target_corrs,
        "vif_scores": vif_data
    }

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
# åŠ è½½æ•°æ®
# df = pd.read_csv('your_data.csv')

# å®šä¹‰ç›®æ ‡åˆ—
# target_cols = ['Nitrogen', 'Phosphorous', 'Potassium']

# å®Œæ•´ç‰¹å¾å·¥ç¨‹
# df_full = feature_engineering_complete(df)

# è¯„ä¼°ç‰¹å¾ (å¯¹æ¯ä¸ªç›®æ ‡åˆ†åˆ«è¯„ä¼°)
# for target in target_cols:
#     print(f"\n{'='*40}")
#     print(f"è¯„ä¼°ç›®æ ‡: {target}")
#     print(f"{'='*40}")
#     results = evaluate_features(df_full, [target], models=['lgb', 'xgb', 'cat'], sample_size=10000)

# æŸ¥çœ‹VIFç»“æœ
# vif_df = results['vif_scores']
# print(vif_df.sort_values('VIF', ascending=False).head(10))




# åŠ è½½æ•°æ®
df = pd.read_csv("data/train.csv")  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®è·¯å¾„

# ç‰¹å¾å·¥ç¨‹å¤„ç†
df_full = feature_engineering_complete(df)

# å®šä¹‰ç›®æ ‡åˆ—ä¸º Fertilizer Name
target_col = ['Fertilizer Name']

# å¯¹ Fertilizer Name è¿›è¡Œæ ‡ç­¾ç¼–ç 
le = LabelEncoder()
df_full['Fertilizer_Name_encoded'] = le.fit_transform(df_full['Fertilizer Name'])

# è°ƒç”¨ç‰¹å¾è¯„ä¼°å‡½æ•°
print("\n==== è¯„ä¼° Fertilizer Name çš„ç‰¹å¾å½±å“ ====")
results = evaluate_features(
    df_full.drop(columns=["Fertilizer Name"]),  # å»æ‰åŸå§‹æ ‡ç­¾åˆ—
    target_columns=['Fertilizer_Name_encoded'],  # ä½¿ç”¨ç¼–ç åçš„æ ‡ç­¾
    models=['lgb', 'xgb', 'cat'],
    sample_size=10000  # å¯ä»¥è°ƒæ•´ä¸ºä½ çš„æ ·æœ¬é‡æˆ– None
)

# æŸ¥çœ‹ VIF ç»“æœ
print("\n==== é«˜ VIF ç‰¹å¾ï¼ˆæ½œåœ¨å¤šé‡å…±çº¿æ€§é—®é¢˜ï¼‰ ====")
vif_df = results['vif_scores']
print(vif_df.sort_values('VIF', ascending=False).head(10))
